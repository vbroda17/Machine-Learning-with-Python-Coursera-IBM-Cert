# Course Outline:
This course has 6 modules, listed below. We encourage you to set aside several hours each week to successfully complete all modules in 4-6 weeks. Consistency will best help you achieve your learning goals!

You will benefit most from viewing all the videos and readings and solidifying that knowledge by completing all the hands-on labs and the final culminating project.

## Module 1: Introduction to Machine Learning

This module provides you with knowledge of foundational machine learning concepts to delve deeper into applied machine learning modeling. You will learn that machine learning modeling is an iterative process with various lifecycle stages. You will also learn about the daily activities of a machine-learning engineer. Here, you will be introduced to various open-source tools for machine learning, including the popular Python package scikit-learn.

## Module 2: Linear and Logistic Regression

This module introduces you to two classical statistical methods foundational to machine learning: linear and logistic regression. You’ll learn how linear regression, pioneered in the 1800s, models linear relationships while logistic regression serves as a classifier. Through implementing these models, you’ll understand their limitations and gain insight into why modern machine-learning models are often preferred.

## Module 3: Building Supervised Learning Models

In this module, you’ll learn about implementing modern supervised machine learning models. You will start by understanding how binary classification works and discover how to construct a multiclass classifier from binary classification components. You’ll learn what decision trees are, how they learn, and how to build them. Decision trees, which are used to solve classification problems, have a natural extension called regression trees, which can handle regression problems. You’ll learn about other supervised learning models, like KNN and SVM. You’ll learn what bias and variance are in model fitting and the tradeoff between bias and variance that is inherent to all learning models in various degrees. You’ll learn strategies for mitigating this tradeoff and work with models that do a very good job accomplishing that goal.

## Module 4: Building Unsupervised Learning Models

In this module, you’ll dive into unsupervised learning, where algorithms uncover patterns in data without labeled examples. You’ll explore clustering strategies and real-world applications, focusing on techniques like hierarchical clustering, k-means, and advanced methods such as DBSCAN and HDBSCAN. Through practical labs, you’ll gain a deeper understanding of how to compare and implement these algorithms effectively. Additionally, you’ll delve into dimension reduction algorithms like PCA (Principal Component Analysis), t-SNE, and UMAP to reduce dataset features and simplify other modeling tasks. Using Python, you’ll implement these clustering and dimensionality reduction techniques, learning how to integrate them with feature engineering to prepare data for machine learning models.

## Module 5: Evaluating and Validating Machine Learning Models

This module covers how to assess model performance on unseen data, starting with key evaluation metrics for classification and regression. You’ll also explore hyperparameter tuning to optimize models while avoiding overfitting using cross-validation. Special techniques, such as regularization in linear regression, will be introduced to handle overfitting due to outliers. Hands-on exercises in Python will guide you through model fine-tuning and cross-validation for reliable model evaluation.

## Module 6: Final Exam and Project

In this concluding module, you’ll review the course content, complete a final exam, and work on a hands-on project. You’ll receive a course summary cheat sheet, apply your skills in a project on Rain Prediction in Australia, and participate in peer reviews to share feedback. The module wraps up with guidance on next steps in your learning journey.